{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alihaydarkir/fine-tuning/blob/main/yeni_veriseti_yeni_fine_tuningi.pynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoMPf99WNjMK",
        "outputId": "a53ca347-8c4c-4d44-8190-f1cbf01c4a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkhqGG6xwTId"
      },
      "source": [
        "\n",
        "# 1. K√úT√úPHANE ƒ∞MPORTLARI VE TEMEL AYARLAR\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnW-M5BNwRHJ",
        "outputId": "98e0e9af-5a46-4675-ad6b-88491f09bcb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñ•Ô∏è  Kullanƒ±lan cihaz: cpu\n",
            "‚ö†Ô∏è  CPU kullanƒ±lƒ±yor\n",
            "‚úÖ T√ºm k√ºt√ºphaneler y√ºklendi!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# PyTorch ve Neural Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# HuggingFace Transformers\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "\n",
        "# Veri i≈üleme\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# G√∂rselle≈ütirme (opsiyonel)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Google Colab i√ßin\n",
        "from google.colab import drive\n",
        "\n",
        "# GPU/CPU otomatik se√ßimi\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è  Kullanƒ±lan cihaz: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  CPU kullanƒ±lƒ±yor\")\n",
        "\n",
        "print(\"‚úÖ T√ºm k√ºt√ºphaneler y√ºklendi!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kdER9KD_U7P"
      },
      "source": [
        "\n",
        "# B√ñL√úM 2: KONFƒ∞G√úRASYON AYARLARI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62nkJeUR_Uia",
        "outputId": "d99baea6-353c-47b7-acf4-4b3fe48f3902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã KONFƒ∞G√úRASYON:\n",
            "Model: gpt2\n",
            "Batch Size: 4\n",
            "Epochs: 3\n",
            "Max Length: 512\n",
            "Learning Rate: 5e-05\n",
            "‚úÖ Konfig√ºrasyon hazƒ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Model ve eƒüitim parametreleri\"\"\"\n",
        "\n",
        "    # Model ayarlarƒ±\n",
        "    model_name = \"gpt2\"              # HuggingFace model adƒ±\n",
        "    max_length = 512                 # Maksimum token uzunluƒüu\n",
        "\n",
        "    # Eƒüitim ayarlarƒ±\n",
        "    batch_size = 4                   # Batch boyutu (GPU memory'e g√∂re)\n",
        "    learning_rate = 5e-5             # √ñƒürenme oranƒ±\n",
        "    num_epochs = 3                   # Epoch sayƒ±sƒ±\n",
        "\n",
        "    # Dosya yollarƒ±\n",
        "    json_file = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/sigort–∞_gpt_ft.json\"\n",
        "    save_path = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/insurance_gpt_model\"\n",
        "\n",
        "    # Text generation ayarlarƒ±\n",
        "    max_new_tokens = 100             # √úretilecek token sayƒ±sƒ±\n",
        "    temperature = 0.7                # Yaratƒ±cƒ±lƒ±k (0.1-1.0)\n",
        "    top_p = 0.9                      # Nucleus sampling\n",
        "    top_k = 50                       # Top-k sampling\n",
        "\n",
        "# Konfig√ºrasyon objesi olu≈ütur\n",
        "config = Config()\n",
        "\n",
        "print(\"üìã KONFƒ∞G√úRASYON:\")\n",
        "print(f\"Model: {config.model_name}\")\n",
        "print(f\"Batch Size: {config.batch_size}\")\n",
        "print(f\"Epochs: {config.num_epochs}\")\n",
        "print(f\"Max Length: {config.max_length}\")\n",
        "print(f\"Learning Rate: {config.learning_rate}\")\n",
        "print(\"‚úÖ Konfig√ºrasyon hazƒ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MEMORY OPTƒ∞Mƒ∞ZE EDƒ∞LMƒ∞≈û MODEL SETUP\n",
        "import gc\n",
        "\n",
        "# Memory temizliƒüi\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# K√ú√á√úK CONFIG\n",
        "class Config:\n",
        "    model_name = \"sberbank-ai/mGPT\"\n",
        "    max_length = 256\n",
        "    batch_size = 1\n",
        "    learning_rate = 2e-5\n",
        "    num_epochs = 1\n",
        "    json_file = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print(f\"üîß Memory optimize ayarlar:\")\n",
        "print(f\"   Max length: {config.max_length}\")\n",
        "print(f\"   Batch size: {config.batch_size}\")\n",
        "print(f\"üíæ Mevcut GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "\n",
        "# Dataset class\n",
        "class InsuranceDataset(Dataset):\n",
        "    def __init__(self, json_file_path, tokenizer, max_length=256):\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        print(f\"üìä Dataset: {len(self.data)} √∂rnek y√ºklendi\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        question = item['input'].strip()\n",
        "        answer = item['output'].strip()\n",
        "        full_text = f\"Soru: {question}\\nCevap: {answer}\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            full_text, truncation=True, padding='max_length',\n",
        "            max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Config ve Dataset hazƒ±r!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnfcJwfwN-NJ",
        "outputId": "a1eab729-d909-4da2-fee5-9f39eab58746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Memory optimize ayarlar:\n",
            "   Max length: 256\n",
            "   Batch size: 1\n",
            "üíæ Mevcut GPU Memory: 0.00 GB\n",
            "‚úÖ Config ve Dataset hazƒ±r!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ng0Hq7waJd"
      },
      "source": [
        "\n",
        "# B√ñL√úM 3: DATASET CLASS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eZCMWmqf37u",
        "outputId": "1ed24e21-09f2-4643-acbe-d3d7d6d56bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset class hazƒ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class InsuranceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Sigorta Q&A verilerini JSON'dan okuyup PyTorch Dataset'e √ßeviren class\n",
        "    Beklenen format: {\"input\": \"soru\", \"output\": \"cevap\"}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, json_file_path, tokenizer, max_length=512):\n",
        "        print(f\"üìÇ JSON dosyasƒ± okunuyor: {json_file_path}\")\n",
        "\n",
        "        # JSON dosyasƒ±nƒ± oku\n",
        "        try:\n",
        "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "                self.data = json.load(f)\n",
        "            print(f\"‚úÖ Toplam {len(self.data)} veri y√ºklendi\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ùå Dosya bulunamadƒ±: {json_file_path}\")\n",
        "            raise\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå JSON format hatasƒ±: {e}\")\n",
        "            raise\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Veri formatƒ±nƒ± kontrol et\n",
        "        if len(self.data) > 0:\n",
        "            sample = self.data[0]\n",
        "            print(f\"üìã Veri anahtarlarƒ±: {list(sample.keys())}\")\n",
        "\n",
        "            # Format kontrol√º\n",
        "            if 'input' in sample and 'output' in sample:\n",
        "                print(f\"‚úÖ Doƒüru format - √ñrnek:\")\n",
        "                print(f\"   Soru: {sample['input'][:50]}...\")\n",
        "                print(f\"   Cevap: {sample['output'][:50]}...\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è  'input' ve 'output' anahtarlarƒ± bulunamadƒ±!\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Veriyi tokenize ederek d√∂nd√ºr\"\"\"\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Soru-cevap formatƒ±nƒ± olu≈ütur\n",
        "        question = item['input'].strip()\n",
        "        answer = item['output'].strip()\n",
        "\n",
        "        # GPT i√ßin training formatƒ±\n",
        "        full_text = f\"Soru: {question}\\nCevap: {answer}\"\n",
        "\n",
        "        # Tokenize et\n",
        "        encoding = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'question': question,  # Debug i√ßin\n",
        "            'answer': answer       # Debug i√ßin\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Dataset class hazƒ±r!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6cUjslPf6aP"
      },
      "source": [
        "# B√ñL√úM 4: TOKENIZER SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzrSRXSuf6HD",
        "outputId": "56d9e717-c9e5-45c3-b6fe-4d97c396fdfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tokenizer setup hazƒ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def setup_tokenizer(model_name):\n",
        "    \"\"\"\n",
        "    HuggingFace tokenizer'ƒ± y√ºkle ve ayarla\n",
        "    \"\"\"\n",
        "    print(f\"üî§ Tokenizer y√ºkleniyor: {model_name}\")\n",
        "\n",
        "    # GPT2 tokenizer y√ºkle\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Padding token ayarla (GPT2'de yoktur)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(\"‚úÖ Padding token ayarlandƒ±\")\n",
        "\n",
        "    # Tokenizer bilgilerini g√∂ster\n",
        "    print(f\"üìä Vocabulary boyutu: {len(tokenizer):,}\")\n",
        "    print(f\"üîö EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
        "    print(f\"üìù PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def test_tokenizer(tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizer'ƒ± test et\n",
        "    \"\"\"\n",
        "    print(\"\\nüß™ TOKENIZER TEST EDƒ∞Lƒ∞YOR...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Test metinleri\n",
        "    test_texts = [\n",
        "        \"Sigorta poli√ßesi nedir?\",\n",
        "        \"Soru: Akt√ºer kimdir?\\nCevap: Sigortacƒ±lƒ±k tekniƒüi uzmanƒ±dƒ±r.\"\n",
        "    ]\n",
        "\n",
        "    for i, text in enumerate(test_texts, 1):\n",
        "        print(f\"\\nüìù Test {i}: {text}\")\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = tokenizer.encode(text)\n",
        "        print(f\"üî¢ Token sayƒ±sƒ±: {len(tokens)}\")\n",
        "        print(f\"üî¢ Token IDs: {tokens}\")\n",
        "\n",
        "        # Decode\n",
        "        decoded = tokenizer.decode(tokens)\n",
        "        print(f\"üìñ Decode: {decoded}\")\n",
        "\n",
        "        # E≈üitlik kontrol√º\n",
        "        if text == decoded:\n",
        "            print(\"‚úÖ Encode/Decode ba≈üarƒ±lƒ±\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è  Decode farklƒ±\")\n",
        "\n",
        "    print(\"‚úÖ Tokenizer test tamamlandƒ±!\")\n",
        "    return True\n",
        "\n",
        "# Tokenizer'ƒ± y√ºkle ve test et\n",
        "def load_and_test_tokenizer(config):\n",
        "    \"\"\"\n",
        "    Tokenizer y√ºkle ve test et\n",
        "    \"\"\"\n",
        "    tokenizer = setup_tokenizer(config.model_name)\n",
        "    test_tokenizer(tokenizer)\n",
        "    return tokenizer\n",
        "\n",
        "print(\"‚úÖ Tokenizer setup hazƒ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGDSfmx3Br6i",
        "outputId": "93e91b6d-f515-4f3b-c870-b555bc0336ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Tokenizer y√ºkleniyor: sberbank-ai/mGPT\n",
            "üìä Vocabulary boyutu: 100,000\n",
            "üîö EOS token: '<|endoftext|>' (ID: 5)\n",
            "üìù PAD token: '<pad>' (ID: 1)\n",
            "\n",
            "üß™ TOKENIZER TEST EDƒ∞Lƒ∞YOR...\n",
            "----------------------------------------\n",
            "\n",
            "üìù Test 1: Sigorta poli√ßesi nedir?\n",
            "üî¢ Token sayƒ±sƒ±: 8\n",
            "üî¢ Token IDs: [52195, 17892, 38377, 739, 3788, 320, 13522, 37]\n",
            "üìñ Decode: Sigorta poli√ßesi nedir?\n",
            "‚úÖ Encode/Decode ba≈üarƒ±lƒ±\n",
            "\n",
            "üìù Test 2: Soru: Akt√ºer kimdir?\n",
            "Cevap: Sigortacƒ±lƒ±k tekniƒüi uzmanƒ±dƒ±r.\n",
            "üî¢ Token sayƒ±sƒ±: 25\n",
            "üî¢ Token IDs: [57, 28073, 32, 15814, 455, 263, 24030, 46055, 37, 205, 41, 836, 511, 32, 19014, 17892, 73, 50345, 6020, 1266, 8704, 11412, 1067, 58753, 20]\n",
            "üìñ Decode: Soru: Akt√ºer kimdir?\n",
            "Cevap: Sigortacƒ±lƒ±k tekniƒüi uzmanƒ±dƒ±r.\n",
            "‚úÖ Encode/Decode ba≈üarƒ±lƒ±\n",
            "‚úÖ Tokenizer test tamamlandƒ±!\n"
          ]
        }
      ],
      "source": [
        "# Tokenizer'ƒ± ger√ßekten y√ºkle ve test et\n",
        "tokenizer = load_and_test_tokenizer(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roEFNdUXf7Q6"
      },
      "source": [
        "# B√ñL√úM 5: MODEL CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH3pLFrWf61H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06f91ee-d77c-46f2-fe36-71a87ac13e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model class hazƒ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class InsuranceGPTModel:\n",
        "    \"\"\"\n",
        "    HuggingFace GPT2 tabanlƒ± sigorta chatbot modeli\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, tokenizer):\n",
        "        \"\"\"\n",
        "        Model ve optimizer'ƒ± initialize et\n",
        "        \"\"\"\n",
        "        print(f\"ü§ñ Model y√ºkleniyor: {config.model_name}\")\n",
        "\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # HuggingFace GPT2 model y√ºkle\n",
        "        try:\n",
        "            self.model = GPT2LMHeadModel.from_pretrained(\n",
        "                config.model_name,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            print(\"‚úÖ Model ba≈üarƒ±yla y√ºklendi\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Model y√ºkleme hatasƒ±: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Token embedding'i resize et\n",
        "        self.model.resize_token_embeddings(len(tokenizer))\n",
        "        print(f\"üìè Token embeddings resize edildi: {len(tokenizer)}\")\n",
        "\n",
        "        # Modeli GPU/CPU'ya ta≈üƒ±\n",
        "        self.model.to(device)\n",
        "        print(f\"üì± Model {device} cihazƒ±na ta≈üƒ±ndƒ±\")\n",
        "\n",
        "        # Model istatistikleri\n",
        "        self._print_model_stats()\n",
        "\n",
        "        # Optimizer hazƒ±rla\n",
        "        self._setup_optimizer()\n",
        "\n",
        "    def _print_model_stats(self):\n",
        "        \"\"\"Model istatistiklerini yazdƒ±r\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "        print(f\"üìä Toplam parametre: {total_params:,}\")\n",
        "        print(f\"üìä Eƒüitilebilir parametre: {trainable_params:,}\")\n",
        "        print(f\"üìä Model boyutu: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
        "\n",
        "    def _setup_optimizer(self):\n",
        "        \"\"\"Optimizer'ƒ± ayarla\"\"\"\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=self.config.learning_rate,\n",
        "            weight_decay=0.01,  # Regularization\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8\n",
        "        )\n",
        "        print(\"‚úÖ Optimizer hazƒ±rlandƒ±\")\n",
        "\n",
        "    def generate_answer(self, question, max_length=150):\n",
        "        \"\"\"\n",
        "        Soruya cevap √ºret\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Prompt olu≈ütur\n",
        "        prompt = f\"Soru: {question}\\nCevap:\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer.encode(\n",
        "            prompt,\n",
        "            return_tensors='pt',\n",
        "            add_special_tokens=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Cevap √ºret\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=len(inputs[0]) + max_length,\n",
        "                temperature=self.config.temperature,\n",
        "                do_sample=True,\n",
        "                top_p=self.config.top_p,\n",
        "                top_k=self.config.top_k,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.2,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "\n",
        "        # Decode ve temizle\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer = full_response.replace(prompt, \"\").strip()\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def save_model(self, save_path=None):\n",
        "        \"\"\"Model ve tokenizer'ƒ± kaydet\"\"\"\n",
        "        if save_path is None:\n",
        "            save_path = self.config.save_path\n",
        "\n",
        "        print(f\"üíæ Model kaydediliyor: {save_path}\")\n",
        "\n",
        "        # Klas√∂r olu≈ütur\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        # Model ve tokenizer kaydet\n",
        "        self.model.save_pretrained(save_path)\n",
        "        self.tokenizer.save_pretrained(save_path)\n",
        "\n",
        "        print(\"‚úÖ Model ba≈üarƒ±yla kaydedildi!\")\n",
        "\n",
        "    def load_model(self, model_path=None):\n",
        "        \"\"\"Kaydedilmi≈ü modeli y√ºkle\"\"\"\n",
        "        if model_path is None:\n",
        "            model_path = self.config.save_path\n",
        "\n",
        "        print(f\"üìÇ Model y√ºkleniyor: {model_path}\")\n",
        "\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.model.to(device)\n",
        "\n",
        "        print(\"‚úÖ Model ba≈üarƒ±yla y√ºklendi!\")\n",
        "\n",
        "def test_model_basic(model):\n",
        "    \"\"\"\n",
        "    Model'in temel √ßalƒ±≈ümasƒ±nƒ± test et\n",
        "    \"\"\"\n",
        "    print(\"\\nüß™ MODEL TEMEL TEST...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    test_questions = [\n",
        "        \"Akt√ºerler Sicili kimde tutulur?\",\n",
        "        \"Sigorta ≈üirketi nedir?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\n‚ùì Soru: {question}\")\n",
        "        try:\n",
        "            answer = model.generate_answer(question, max_length=50)\n",
        "            print(f\"ü§ñ Cevap: {answer}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Hata: {e}\")\n",
        "\n",
        "    print(\"‚úÖ Model temel test tamamlandƒ±!\")\n",
        "\n",
        "print(\"‚úÖ Model class hazƒ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGyRbM4FB-pL"
      },
      "outputs": [],
      "source": [
        "# Google Drive'ƒ± mount et (eƒüer etmediysen)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Model class'ƒ±nƒ± test et\n",
        "model = InsuranceGPTModel(config, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6UUotcEf8jj"
      },
      "source": [
        "# B√ñL√úM 6: TRAINING FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pThSTEgef8Ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1c0c00-8413-42d8-db76-0149e7b9f040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training functions hazƒ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, epoch_num, config):\n",
        "    \"\"\"\n",
        "    Tek epoch eƒüitimi\n",
        "    \"\"\"\n",
        "    model.model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    print(f\"üéØ Epoch {epoch_num+1}/{config.num_epochs} ba≈ülƒ±yor...\")\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        dataloader,\n",
        "        desc=f'Epoch {epoch_num+1}',\n",
        "        leave=True\n",
        "    )\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Veriyi GPU'ya ta≈üƒ±\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=input_ids  # Language modeling i√ßin\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        model.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (exploding gradient'larƒ± √∂nle)\n",
        "        torch.nn.utils.clip_grad_norm_(model.model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Optimizer step\n",
        "        model.optimizer.step()\n",
        "\n",
        "        # Loss tracking\n",
        "        total_loss += loss.item()\n",
        "        current_avg_loss = total_loss / (batch_idx + 1)\n",
        "\n",
        "        # Progress bar g√ºncelle\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Avg': f'{current_avg_loss:.4f}',\n",
        "            'GPU': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB' if torch.cuda.is_available() else 'CPU'\n",
        "        })\n",
        "\n",
        "        # Memory temizliƒüi (her 10 batch'te bir)\n",
        "        if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_avg_loss = total_loss / num_batches\n",
        "    print(f\"‚úÖ Epoch {epoch_num+1} tamamlandƒ± - Avg Loss: {epoch_avg_loss:.4f}\")\n",
        "\n",
        "    return epoch_avg_loss\n",
        "\n",
        "def full_training(model, dataset, config):\n",
        "    \"\"\"\n",
        "    Tam eƒüitim d√∂ng√ºs√º\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üöÄ Eƒûƒ∞Tƒ∞M BA≈ûLIYOR...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # DataLoader olu≈ütur\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"üìä Toplam batch sayƒ±sƒ±: {len(dataloader)}\")\n",
        "    print(f\"üìä Her epoch'ta ~{len(dataset)} √∂rnek\")\n",
        "\n",
        "    # Eƒüitim d√∂ng√ºs√º\n",
        "    losses = []\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f\"\\n{'='*20} EPOCH {epoch+1}/{config.num_epochs} {'='*20}\")\n",
        "\n",
        "        # Epoch eƒüitimi\n",
        "        epoch_loss = train_one_epoch(model, dataloader, epoch, config)\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "        # En iyi model tracking\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            print(f\"üèÜ Yeni en iyi loss: {best_loss:.4f}\")\n",
        "\n",
        "        # Memory temizliƒüi\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\nüèÅ Eƒûƒ∞Tƒ∞M TAMAMLANDI!\")\n",
        "    print(f\"üìà Son loss: {losses[-1]:.4f}\")\n",
        "    print(f\"üèÜ En iyi loss: {best_loss:.4f}\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "def plot_training_loss(losses):\n",
        "    \"\"\"\n",
        "    Training loss grafiƒüini √ßiz\n",
        "    \"\"\"\n",
        "    if len(losses) == 0:\n",
        "        print(\"‚ö†Ô∏è  √áizilebilecek loss verisi yok\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(losses) + 1), losses, 'b-', marker='o', linewidth=2, markersize=8)\n",
        "    plt.title('Training Loss', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(range(1, len(losses) + 1))\n",
        "\n",
        "    # Loss deƒüerlerini grafikte g√∂ster\n",
        "    for i, loss in enumerate(losses):\n",
        "        plt.annotate(f'{loss:.4f}', (i+1, loss), textcoords=\"offset points\",\n",
        "                    xytext=(0,10), ha='center', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"üìä Loss grafiƒüi √ßizildi!\")\n",
        "\n",
        "print(\"‚úÖ Training functions hazƒ±r!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNFWb6kgRoi"
      },
      "source": [
        "# B√ñL√úM 7: TEST FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOjhTtw-gRXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f429023-0467-4fd9-934b-61ab2c3c5357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Test functions hazƒ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def test_dataset_loading(json_file_path, tokenizer, config):\n",
        "    \"\"\"\n",
        "    Dataset y√ºkleme ve tokenizasyon testleri\n",
        "    \"\"\"\n",
        "    print(\"\\nüß™ DATASET Y√úKLEMESƒ∞ TEST EDƒ∞Lƒ∞YOR...\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    try:\n",
        "        # Dataset olu≈ütur\n",
        "        dataset = InsuranceDataset(json_file_path, tokenizer, config.max_length)\n",
        "\n",
        "        if len(dataset) == 0:\n",
        "            print(\"‚ùå Dataset bo≈ü!\")\n",
        "            return False\n",
        "\n",
        "        # ƒ∞lk √∂rneƒüi kontrol et\n",
        "        print(f\"‚úÖ Dataset boyutu: {len(dataset)}\")\n",
        "\n",
        "        # Tokenizasyon testi\n",
        "        sample = dataset[0]\n",
        "        print(f\"‚úÖ Input IDs shape: {sample['input_ids'].shape}\")\n",
        "        print(f\"‚úÖ Attention mask shape: {sample['attention_mask'].shape}\")\n",
        "\n",
        "        # √ñrnek decode\n",
        "        decoded = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
        "        print(f\"üìù ƒ∞lk √∂rnek: {decoded[:100]}...\")\n",
        "\n",
        "        # DataLoader testi\n",
        "        dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
        "        batch = next(iter(dataloader))\n",
        "        print(f\"‚úÖ Batch test - Input shape: {batch['input_ids'].shape}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Dataset test hatasƒ±: {e}\")\n",
        "        return False\n",
        "\n",
        "def test_model_generation(model, test_questions):\n",
        "    \"\"\"\n",
        "    Model text generation testleri\n",
        "    \"\"\"\n",
        "    print(\"\\nüß™ MODEL GENERATƒ∞ON TEST EDƒ∞Lƒ∞YOR...\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\nüî∏ Test {i}/{len(test_questions)}\")\n",
        "        print(f\"‚ùì Soru: {question}\")\n",
        "\n",
        "        try:\n",
        "            # Cevap √ºret\n",
        "            answer = model.generate_answer(question, max_length=80)\n",
        "            print(f\"ü§ñ Cevap: {answer}\")\n",
        "\n",
        "            # Token analizi\n",
        "            prompt = f\"Soru: {question}\\nCevap:\"\n",
        "            tokens = model.tokenizer.encode(prompt)\n",
        "            print(f\"üî¢ Prompt token sayƒ±sƒ±: {len(tokens)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Generation hatasƒ±: {e}\")\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "def compare_before_after_training(model, test_questions):\n",
        "    \"\"\"\n",
        "    Eƒüitim √∂ncesi ve sonrasƒ± kar≈üƒ±la≈ütƒ±rmasƒ±\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä Eƒûƒ∞Tƒ∞M √ñNCESƒ∞/SONRASI KAR≈ûILA≈ûTIRMA\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\n‚ùì Soru: {question}\")\n",
        "\n",
        "        # Eƒüitim √∂ncesi cevap (bu fonksiyon eƒüitim sonrasƒ± √ßaƒürƒ±lƒ±r)\n",
        "        answer = model.generate_answer(question, max_length=60)\n",
        "\n",
        "        results[question] = {\n",
        "            'after_training': answer\n",
        "        }\n",
        "\n",
        "        print(f\"ü§ñ Eƒüitim sonrasƒ±: {answer}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_model_performance(model, dataset, sample_size=10):\n",
        "    \"\"\"\n",
        "    Model performansƒ±nƒ± deƒüerlendir\n",
        "    \"\"\"\n",
        "    print(f\"\\nüìà MODEL PERFORMANS DEƒûERLENDƒ∞RME (Sample: {sample_size})\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Random sample al\n",
        "    import random\n",
        "    indices = random.sample(range(len(dataset)), min(sample_size, len(dataset)))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        sample = dataset[idx]\n",
        "        question = sample['question']\n",
        "        expected_answer = sample['answer']\n",
        "\n",
        "        print(f\"\\nüìù Test {i+1}/{sample_size}\")\n",
        "        print(f\"‚ùì Soru: {question}\")\n",
        "        print(f\"‚úÖ Beklenen: {expected_answer[:50]}...\")\n",
        "\n",
        "        # Model cevabƒ±\n",
        "        generated_answer = model.generate_answer(question, max_length=60)\n",
        "        print(f\"ü§ñ √úretilen: {generated_answer[:50]}...\")\n",
        "\n",
        "        # Basit benzerlik skoru (kelime overlap)\n",
        "        expected_words = set(expected_answer.lower().split())\n",
        "        generated_words = set(generated_answer.lower().split())\n",
        "\n",
        "        if len(expected_words) > 0:\n",
        "            overlap = len(expected_words.intersection(generated_words))\n",
        "            similarity = overlap / len(expected_words)\n",
        "        else:\n",
        "            similarity = 0\n",
        "\n",
        "        print(f\"üìä Benzerlik: {similarity:.2f}\")\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'expected': expected_answer,\n",
        "            'generated': generated_answer,\n",
        "            'similarity': similarity\n",
        "        })\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # Ortalama performans\n",
        "    avg_similarity = sum(r['similarity'] for r in results) / len(results)\n",
        "    print(f\"\\nüèÜ ORTALAMA BENZERLƒ∞K: {avg_similarity:.3f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def interactive_test_mode(model):\n",
        "    \"\"\"\n",
        "    ƒ∞nteraktif test modu\n",
        "    \"\"\"\n",
        "    print(\"\\nüéÆ ƒ∞NTERAKTƒ∞F TEST MODU\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Sigorta ile ilgili sorularƒ±nƒ±zƒ± sorun!\")\n",
        "    print(\"√áƒ±kmak i√ßin 'quit', 'exit' veya '√ßƒ±k' yazƒ±n.\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            question = input(\"\\n‚ùì Sigorta sorunuz: \").strip()\n",
        "\n",
        "            # √áƒ±kƒ±≈ü kontrolleri\n",
        "            if question.lower() in ['quit', 'exit', '√ßƒ±k', 'q']:\n",
        "                print(\"üëã ƒ∞nteraktif test sonlandƒ±rƒ±lƒ±yor...\")\n",
        "                break\n",
        "\n",
        "            if not question:\n",
        "                print(\"‚ö†Ô∏è  Bo≈ü soru girdiniz!\")\n",
        "                continue\n",
        "\n",
        "            # Tokenizasyon bilgisi\n",
        "            tokens = model.tokenizer.encode(f\"Soru: {question}\\nCevap:\")\n",
        "            print(f\"üî¢ Token sayƒ±sƒ±: {len(tokens)}\")\n",
        "\n",
        "            # Cevap √ºret\n",
        "            print(\"ü§ñ D√º≈ü√ºn√ºyorum...\")\n",
        "            answer = model.generate_answer(question, max_length=100)\n",
        "\n",
        "            print(f\"ü§ñ Cevap: {answer}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nüëã Kullanƒ±cƒ± √ßƒ±kƒ±≈üƒ± yapƒ±ldƒ±.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Hata: {e}\")\n",
        "\n",
        "# Test soru setleri\n",
        "BASIC_TEST_QUESTIONS = [\n",
        "    \"Akt√ºerler Sicili kimde tutulur?\",\n",
        "    \"Brokerlik ruhsatƒ± kimden alƒ±nƒ±r?\",\n",
        "    \"Minimum garanti fonu nedir?\"\n",
        "]\n",
        "\n",
        "ADVANCED_TEST_QUESTIONS = [\n",
        "    \"Sigorta ≈üirketleri hangi bran≈ülarda faaliyet g√∂sterebilir?\",\n",
        "    \"Teknik kar≈üƒ±lƒ±klar nelerdir?\",\n",
        "    \"Zorunlu sigortalar kimler tarafƒ±ndan ihdas edilir?\",\n",
        "    \"G√ºvence Hesabƒ± ne i≈üe yarar?\",\n",
        "    \"Sigorta eksperi kimdir?\"\n",
        "]\n",
        "\n",
        "def run_all_tests(model, dataset):\n",
        "    \"\"\"\n",
        "    T√ºm testleri √ßalƒ±≈ütƒ±r\n",
        "    \"\"\"\n",
        "    print(\"\\nüß™ T√úM TESTLER √áALI≈ûTIRILIYOR...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Temel generation testleri\n",
        "    test_model_generation(model, BASIC_TEST_QUESTIONS)\n",
        "\n",
        "    # Performans deƒüerlendirmesi\n",
        "    evaluate_model_performance(model, dataset, sample_size=5)\n",
        "\n",
        "    # ƒ∞nteraktif mod\n",
        "    interactive_test_mode(model)\n",
        "\n",
        "print(\"‚úÖ Test functions hazƒ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ofqTrZBECgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3c1f00-5d38-41fd-dcef-80cae8a247a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Yeni dosya yolu: /content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\n",
            "\n",
            "üß™ DATASET Y√úKLEMESƒ∞ TEST EDƒ∞Lƒ∞YOR...\n",
            "---------------------------------------------\n",
            "üìÇ JSON dosyasƒ± okunuyor: /content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\n",
            "‚úÖ Toplam 89 veri y√ºklendi\n",
            "üìã Veri anahtarlarƒ±: ['input', 'output']\n",
            "‚úÖ Doƒüru format - √ñrnek:\n",
            "   Soru: Bu kanunun amacƒ± nedir?...\n",
            "   Cevap: Bu Kanunun amacƒ±, √ºlkemiz sigortacƒ±lƒ±ƒüƒ±nƒ±n geli≈üti...\n",
            "‚úÖ Dataset boyutu: 89\n",
            "‚úÖ Input IDs shape: torch.Size([256])\n",
            "‚úÖ Attention mask shape: torch.Size([256])\n",
            "üìù ƒ∞lk √∂rnek: Soru: Bu kanunun amacƒ± nedir?\n",
            "Cevap: Bu Kanunun amacƒ±, √ºlkemiz sigortacƒ±lƒ±ƒüƒ±nƒ±n geli≈ütirilmesini saƒü...\n",
            "‚úÖ Batch test - Input shape: torch.Size([2, 256])\n",
            "Dataset test sonucu: True\n"
          ]
        }
      ],
      "source": [
        "# Doƒüru dosya yolunu config'e kaydet\n",
        "config.json_file = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\"\n",
        "\n",
        "print(f\"‚úÖ Yeni dosya yolu: {config.json_file}\")\n",
        "\n",
        "# ≈ûimdi dataset testini tekrar yapalƒ±m\n",
        "dataset_test_result = test_dataset_loading(config.json_file, tokenizer, config)\n",
        "print(f\"Dataset test sonucu: {dataset_test_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPYI0v4uzJbe"
      },
      "source": [
        "\n",
        "# B√ñL√úM 8: MAIN EXECUTION PIPELINE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2-SkoSeBhb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1c5470-44fa-48ec-f498-64e67df79d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Main execution pipeline hazƒ±r!\n",
            "\n",
            "üìã KULLANIM:\n",
            "‚Ä¢ quick_test() - Hƒ±zlƒ± test (1 epoch)\n",
            "‚Ä¢ full_training_run() - Tam eƒüitim\n",
            "‚Ä¢ only_test_model() - Sadece test\n",
            "‚Ä¢ main_pipeline(quick_mode=True/False) - Manual kontrol\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def setup_google_drive():\n",
        "    \"\"\"\n",
        "    Google Drive'ƒ± mount et\n",
        "    \"\"\"\n",
        "    print(\"üìÅ Google Drive kontrol ediliyor...\")\n",
        "\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"üîó Google Drive mount ediliyor...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"‚úÖ Google Drive mount edildi!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Google Drive zaten mount edilmi≈ü!\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def check_file_exists(file_path):\n",
        "    \"\"\"\n",
        "    Dosya varlƒ±ƒüƒ±nƒ± kontrol et\n",
        "    \"\"\"\n",
        "    print(f\"üìÇ Dosya kontrol ediliyor: {file_path}\")\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
        "        print(f\"‚úÖ Dosya bulundu! Boyut: {file_size:.1f} KB\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"‚ùå Dosya bulunamadƒ±: {file_path}\")\n",
        "        return False\n",
        "\n",
        "def main_pipeline(quick_mode=False):\n",
        "    \"\"\"\n",
        "    Ana eƒüitim pipeline'ƒ±\n",
        "    \"\"\"\n",
        "    print(\"üöÄ Sƒ∞GORTA GPT Eƒûƒ∞Tƒ∞M Pƒ∞PELƒ∞NE BA≈ûLIYOR!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Hƒ±zlƒ± mod ayarlarƒ±\n",
        "        if quick_mode:\n",
        "            print(\"‚ö° HIZLI MOD AKTƒ∞F - Test i√ßin optimize edildi\")\n",
        "            config.num_epochs = 1\n",
        "            config.batch_size = 2\n",
        "\n",
        "        # ADIM 1: Google Drive Setup\n",
        "        print(f\"\\n{'='*15} ADIM 1: DRIVE SETUP {'='*15}\")\n",
        "        setup_google_drive()\n",
        "\n",
        "        # ADIM 2: Dosya Kontrolleri\n",
        "        print(f\"\\n{'='*15} ADIM 2: DOSYA KONTROL {'='*15}\")\n",
        "        if not check_file_exists(config.json_file):\n",
        "            print(\"‚ùå Pipeline durduruluyor - JSON dosyasƒ± bulunamadƒ±!\")\n",
        "            return False\n",
        "\n",
        "        # ADIM 3: Tokenizer Y√ºkleme\n",
        "        print(f\"\\n{'='*15} ADIM 3: TOKENIZER {'='*15}\")\n",
        "        tokenizer = load_and_test_tokenizer(config)\n",
        "\n",
        "        # ADIM 4: Dataset Test\n",
        "        print(f\"\\n{'='*15} ADIM 4: DATASET TEST {'='*15}\")\n",
        "        if not test_dataset_loading(config.json_file, tokenizer, config):\n",
        "            print(\"‚ùå Pipeline durduruluyor - Dataset hatasƒ±!\")\n",
        "            return False\n",
        "\n",
        "        # ADIM 5: Model Y√ºkleme\n",
        "        print(f\"\\n{'='*15} ADIM 5: MODEL Y√úKLEME {'='*15}\")\n",
        "        model = InsuranceGPTModel(config, tokenizer)\n",
        "\n",
        "        # ADIM 6: Eƒüitim √ñncesi Test\n",
        "        print(f\"\\n{'='*15} ADIM 6: √ñNCESƒ∞ TEST {'='*15}\")\n",
        "        print(\"üìù Eƒüitim √∂ncesi model cevaplarƒ±:\")\n",
        "        test_model_generation(model, BASIC_TEST_QUESTIONS[:2])\n",
        "\n",
        "        # ADIM 7: Dataset Y√ºkleme\n",
        "        print(f\"\\n{'='*15} ADIM 7: DATASET Y√úKLEME {'='*15}\")\n",
        "        dataset = InsuranceDataset(config.json_file, tokenizer, config.max_length)\n",
        "        print(f\"‚úÖ {len(dataset)} eƒüitim √∂rneƒüi hazƒ±r!\")\n",
        "\n",
        "        # ADIM 8: Model Eƒüitimi\n",
        "        print(f\"\\n{'='*15} ADIM 8: MODEL Eƒûƒ∞Tƒ∞Mƒ∞ {'='*15}\")\n",
        "        losses = full_training(model, dataset, config)\n",
        "\n",
        "        # ADIM 9: Loss Grafiƒüi\n",
        "        print(f\"\\n{'='*15} ADIM 9: LOSS GRAFƒ∞ƒûƒ∞ {'='*15}\")\n",
        "        plot_training_loss(losses)\n",
        "\n",
        "        # ADIM 10: Eƒüitim Sonrasƒ± Test\n",
        "        print(f\"\\n{'='*15} ADIM 10: SONRASI TEST {'='*15}\")\n",
        "        print(\"üìù Eƒüitim sonrasƒ± model cevaplarƒ±:\")\n",
        "        test_model_generation(model, BASIC_TEST_QUESTIONS)\n",
        "\n",
        "        # ADIM 11: Model Kaydetme\n",
        "        print(f\"\\n{'='*15} ADIM 11: MODEL KAYDET {'='*15}\")\n",
        "        model.save_model()\n",
        "\n",
        "        # ADIM 12: Performans Deƒüerlendirme\n",
        "        print(f\"\\n{'='*15} ADIM 12: PERFORMANS {'='*15}\")\n",
        "        evaluate_model_performance(model, dataset, sample_size=3)\n",
        "\n",
        "        print(\"\\nüéâ T√úM ADIMLAR BA≈ûARIYLA TAMAMLANDI!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # ƒ∞nteraktif mod teklifi\n",
        "        user_input = input(\"\\nüéÆ ƒ∞nteraktif test moduna ge√ßmek istiyor musunuz? (e/h): \").lower()\n",
        "        if user_input in ['e', 'evet', 'y', 'yes']:\n",
        "            interactive_test_mode(model)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Pƒ∞PELƒ∞NE HATASI: {e}\")\n",
        "        import traceback\n",
        "        print(\"\\nDetaylƒ± hata:\")\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"\n",
        "    Hƒ±zlƒ± test modu - 1 epoch, k√º√ß√ºk batch\n",
        "    \"\"\"\n",
        "    print(\"‚ö° HIZLI TEST MODU BA≈ûLIYOR...\")\n",
        "    return main_pipeline(quick_mode=True)\n",
        "\n",
        "def full_training_run():\n",
        "    \"\"\"\n",
        "    Tam eƒüitim modu - t√ºm epoch'lar\n",
        "    \"\"\"\n",
        "    print(\"üöÄ TAM Eƒûƒ∞Tƒ∞M MODU BA≈ûLIYOR...\")\n",
        "    return main_pipeline(quick_mode=False)\n",
        "\n",
        "def only_test_model(model_path=None):\n",
        "    \"\"\"\n",
        "    Sadece kaydedilmi≈ü modeli test et\n",
        "    \"\"\"\n",
        "    print(\"üß™ SADECE MODEL TEST MODU\")\n",
        "\n",
        "    if model_path is None:\n",
        "        model_path = config.save_path\n",
        "\n",
        "    # Tokenizer ve model y√ºkle\n",
        "    tokenizer = load_and_test_tokenizer(config)\n",
        "    model = InsuranceGPTModel(config, tokenizer)\n",
        "    model.load_model(model_path)\n",
        "\n",
        "    # Testleri √ßalƒ±≈ütƒ±r\n",
        "    test_model_generation(model, ADVANCED_TEST_QUESTIONS)\n",
        "    interactive_test_mode(model)\n",
        "\n",
        "# Kullanƒ±m kƒ±lavuzu\n",
        "print(\"‚úÖ Main execution pipeline hazƒ±r!\")\n",
        "print(\"\\nüìã KULLANIM:\")\n",
        "print(\"‚Ä¢ quick_test() - Hƒ±zlƒ± test (1 epoch)\")\n",
        "print(\"‚Ä¢ full_training_run() - Tam eƒüitim\")\n",
        "print(\"‚Ä¢ only_test_model() - Sadece test\")\n",
        "print(\"‚Ä¢ main_pipeline(quick_mode=True/False) - Manual kontrol\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIwPb1fxEqGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc94959-ecde-484a-a68a-1c7198f24612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Tokenizer y√ºkleniyor: sberbank-ai/mGPT\n",
            "ü§ñ Model y√ºkleniyor: sberbank-ai/mGPT\n",
            "üìä Dataset olu≈üturuluyor...\n",
            "‚úÖ 89 √∂rnek hazƒ±r!\n",
            "\n",
            "üß™ Eƒûƒ∞Tƒ∞M √ñNCESƒ∞ TEST:\n",
            "‚ùì Akt√ºerler Sicili kimde tutulur?\n",
            "ü§ñ Akt√ºerler Sicili, Akt√ºerlik mesleƒüinin gerektirdiƒüi bilgi ve beceriler kazanmak amacƒ±yla, meslek mensuplarƒ± arasƒ±nda yapƒ±lan ara≈ütƒ±rmalar sonucunda, gerekli ≈üartlara sahip\n",
            "------------------------------\n",
            "‚ùì Brokerlik ruhsatƒ± kimden alƒ±nƒ±r?\n",
            "ü§ñ Brokerlik ruhsatƒ±, brokerlere verilir. Bu ruhsat, brokerlerce verilir. Bu ruhsatlar, brokerlerin genel g√∂revleri arasƒ±nda yer alƒ±r.\n",
            "Soru: Kƒ±saca ne i≈ü yapar\n",
            "------------------------------\n",
            "‚úÖ T√ºm setup tamamlandƒ±! ≈ûimdi eƒüitime ge√ßebiliriz.\n"
          ]
        }
      ],
      "source": [
        "# T√úM KODLARI TEK H√úCREDE TOPARLAYALIM\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Config\n",
        "class Config:\n",
        "    model_name = \"sberbank-ai/mGPT\"\n",
        "    max_length = 512\n",
        "    batch_size = 2\n",
        "    learning_rate = 2e-5\n",
        "    num_epochs = 1\n",
        "    json_file = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\"\n",
        "\n",
        "config = Config()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Dataset Class\n",
        "class InsuranceDataset(Dataset):\n",
        "    def __init__(self, json_file_path, tokenizer, max_length=512):\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        question = item['input'].strip()\n",
        "        answer = item['output'].strip()\n",
        "        full_text = f\"Soru: {question}\\nCevap: {answer}\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            full_text, truncation=True, padding='max_length',\n",
        "            max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'question': question, 'answer': answer\n",
        "        }\n",
        "\n",
        "# Model ve tokenizer y√ºkle\n",
        "print(f\"üî§ Tokenizer y√ºkleniyor: {config.model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"ü§ñ Model y√ºkleniyor: {config.model_name}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(config.model_name).to(device)\n",
        "\n",
        "# Text generation fonksiyonu\n",
        "def generate_text(model, tokenizer, prompt, max_length=50):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs, max_length=len(inputs[0]) + max_length,\n",
        "                                temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
        "\n",
        "# Dataset olu≈ütur\n",
        "print(\"üìä Dataset olu≈üturuluyor...\")\n",
        "dataset = InsuranceDataset(config.json_file, tokenizer, config.max_length)\n",
        "print(f\"‚úÖ {len(dataset)} √∂rnek hazƒ±r!\")\n",
        "\n",
        "# Eƒüitim √∂ncesi test\n",
        "print(\"\\nüß™ Eƒûƒ∞Tƒ∞M √ñNCESƒ∞ TEST:\")\n",
        "test_questions = [\"Akt√ºerler Sicili kimde tutulur?\", \"Brokerlik ruhsatƒ± kimden alƒ±nƒ±r?\"]\n",
        "for q in test_questions:\n",
        "    prompt = f\"Soru: {q}\\nCevap:\"\n",
        "    answer = generate_text(model, tokenizer, prompt, 50)\n",
        "    print(f\"‚ùì {q}\")\n",
        "    print(f\"ü§ñ {answer}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"‚úÖ T√ºm setup tamamlandƒ±! ≈ûimdi eƒüitime ge√ßebiliriz.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL VE TOKENIZER Y√úKLEME\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(\"üî§ Tokenizer y√ºkleniyor...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"‚úÖ Padding token ayarlandƒ±\")\n",
        "\n",
        "print(f\"üìä Vocabulary: {len(tokenizer):,} token\")\n",
        "\n",
        "print(\"ü§ñ Model y√ºkleniyor (float16 - memory efficient)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name,\n",
        "    torch_dtype=torch.float16  # Memory i√ßin float16\n",
        ").to(device)\n",
        "\n",
        "print(f\"üíæ Model y√ºklendi - GPU: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(model, tokenizer, prompt, max_length=40):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=len(inputs[0]) + max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
        "\n",
        "print(\"‚úÖ Model setup tamamlandƒ±!\")\n",
        "\n",
        "# Hƒ±zlƒ± test\n",
        "print(\"\\nüß™ HIZLI TEST:\")\n",
        "test_q = \"Sigorta nedir?\"\n",
        "prompt = f\"Soru: {test_q}\\nCevap:\"\n",
        "answer = generate_text(model, tokenizer, prompt, 30)\n",
        "print(f\"‚ùì {test_q}\")\n",
        "print(f\"ü§ñ {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "k2_Z64jmUef4",
        "outputId": "904ed4b9-64d5-49c4-944d-c3ed7d4485e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Tokenizer y√ºkleniyor...\n",
            "üìä Vocabulary: 100,000 token\n",
            "ü§ñ Model y√ºkleniyor (float16 - memory efficient)...\n",
            "üíæ Model y√ºklendi - GPU: 0.00 GB\n",
            "‚úÖ Model setup tamamlandƒ±!\n",
            "\n",
            "üß™ HIZLI TEST:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-373971593.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtest_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Sigorta nedir?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Soru: {test_q}\\nCevap:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚ùì {test_q}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ü§ñ {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-373971593.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, prompt, max_length)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2629\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2630\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2631\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3611\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3613\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             outputs = block(\n\u001b[0m\u001b[1;32m    928\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0mpast_key_values\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, past_key_value, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esWCQB6GzYjL"
      },
      "source": [
        "# 12. Hƒ±zlƒ± √áalƒ±≈ütƒ±rma Fonksiyonlarƒ±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlQrmYVeH15O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a338c1ac-aeee-430f-f903-60c592ce6ed9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "üéØ Eƒûƒ∞Tƒ∞M BA≈ûLIYOR!\n",
            "üìä Epoch: 1 | Batch: 45 | LR: 2e-05\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/1:   0%|          | 0/45 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# Eƒûƒ∞Tƒ∞M FONKSƒ∞YONU VE BA≈ûLATMA\n",
        "def train_model(model, dataset, config):\n",
        "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    model.train()\n",
        "    print(f\"üéØ Eƒûƒ∞Tƒ∞M BA≈ûLIYOR!\")\n",
        "    print(f\"üìä Epoch: {config.num_epochs} | Batch: {len(dataloader)} | LR: {config.learning_rate}\")\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{config.num_epochs}')\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'avg': f'{total_loss/(progress_bar.n+1):.4f}'\n",
        "            })\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"‚úÖ Epoch {epoch+1} tamamlandƒ± - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Eƒûƒ∞Tƒ∞Mƒ∞ BA≈ûLAT\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "trained_model = train_model(model, dataset, config)\n",
        "\n",
        "# Eƒûƒ∞Tƒ∞M SONRASI TEST\n",
        "print(\"\\nüß™ Eƒûƒ∞Tƒ∞M SONRASI TEST:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for question in test_questions:\n",
        "    prompt = f\"Soru: {question}\\nCevap:\"\n",
        "    answer = generate_text(trained_model, tokenizer, prompt, max_length=60)\n",
        "    print(f\"‚ùì {question}\")\n",
        "    print(f\"ü§ñ Eƒûƒ∞Tƒ∞M SONRASI: {answer}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"üéâ Eƒûƒ∞Tƒ∞M VE TEST TAMAMLANDI!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rkhqGG6xwTId",
        "6kdER9KD_U7P",
        "A3ng0Hq7waJd",
        "O6cUjslPf6aP",
        "roEFNdUXf7Q6",
        "G6UUotcEf8jj"
      ],
      "provenance": [],
      "mount_file_id": "10sA0sJ5uxFYKE5s5rPbNVCFYo9W9U2Kw",
      "authorship_tag": "ABX9TyOC3J3yrP4by75xaiSlU084",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}