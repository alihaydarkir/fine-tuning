{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alihaydarkir/fine-tuning/blob/main/yeni_veriseti_yeni_fine_tuningi.pynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoMPf99WNjMK",
        "outputId": "a53ca347-8c4c-4d44-8190-f1cbf01c4a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkhqGG6xwTId"
      },
      "source": [
        "\n",
        "# 1. KÃœTÃœPHANE Ä°MPORTLARI VE TEMEL AYARLAR\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnW-M5BNwRHJ",
        "outputId": "98e0e9af-5a46-4675-ad6b-88491f09bcb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ–¥ï¸  KullanÄ±lan cihaz: cpu\n",
            "âš ï¸  CPU kullanÄ±lÄ±yor\n",
            "âœ… TÃ¼m kÃ¼tÃ¼phaneler yÃ¼klendi!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# PyTorch ve Neural Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# HuggingFace Transformers\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
        "\n",
        "# Veri iÅŸleme\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# GÃ¶rselleÅŸtirme (opsiyonel)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Google Colab iÃ§in\n",
        "from google.colab import drive\n",
        "\n",
        "# GPU/CPU otomatik seÃ§imi\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ–¥ï¸  KullanÄ±lan cihaz: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸš€ GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  CPU kullanÄ±lÄ±yor\")\n",
        "\n",
        "print(\"âœ… TÃ¼m kÃ¼tÃ¼phaneler yÃ¼klendi!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kdER9KD_U7P"
      },
      "source": [
        "\n",
        "# BÃ–LÃœM 2: KONFÄ°GÃœRASYON AYARLARI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62nkJeUR_Uia",
        "outputId": "d99baea6-353c-47b7-acf4-4b3fe48f3902"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‹ KONFÄ°GÃœRASYON:\n",
            "Model: gpt2\n",
            "Batch Size: 4\n",
            "Epochs: 3\n",
            "Max Length: 512\n",
            "Learning Rate: 5e-05\n",
            "âœ… KonfigÃ¼rasyon hazÄ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Model ve eÄŸitim parametreleri\"\"\"\n",
        "\n",
        "    # Model ayarlarÄ±\n",
        "    model_name = \"gpt2\"              # HuggingFace model adÄ±\n",
        "    max_length = 512                 # Maksimum token uzunluÄŸu\n",
        "\n",
        "    # EÄŸitim ayarlarÄ±\n",
        "    batch_size = 4                   # Batch boyutu (GPU memory'e gÃ¶re)\n",
        "    learning_rate = 5e-5             # Ã–ÄŸrenme oranÄ±\n",
        "    num_epochs = 3                   # Epoch sayÄ±sÄ±\n",
        "\n",
        "    # Dosya yollarÄ±\n",
        "    json_file = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/sigortĞ°_gpt_ft.json\"\n",
        "    save_path = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/insurance_gpt_model\"\n",
        "\n",
        "    # Text generation ayarlarÄ±\n",
        "    max_new_tokens = 100             # Ãœretilecek token sayÄ±sÄ±\n",
        "    temperature = 0.7                # YaratÄ±cÄ±lÄ±k (0.1-1.0)\n",
        "    top_p = 0.9                      # Nucleus sampling\n",
        "    top_k = 50                       # Top-k sampling\n",
        "\n",
        "# KonfigÃ¼rasyon objesi oluÅŸtur\n",
        "config = Config()\n",
        "\n",
        "print(\"ğŸ“‹ KONFÄ°GÃœRASYON:\")\n",
        "print(f\"Model: {config.model_name}\")\n",
        "print(f\"Batch Size: {config.batch_size}\")\n",
        "print(f\"Epochs: {config.num_epochs}\")\n",
        "print(f\"Max Length: {config.max_length}\")\n",
        "print(f\"Learning Rate: {config.learning_rate}\")\n",
        "print(\"âœ… KonfigÃ¼rasyon hazÄ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MEMORY OPTÄ°MÄ°ZE EDÄ°LMÄ°Å MODEL SETUP\n",
        "import gc\n",
        "\n",
        "# Memory temizliÄŸi\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# KÃœÃ‡ÃœK CONFIG\n",
        "class Config:\n",
        "    model_name = \"sberbank-ai/mGPT\"\n",
        "    max_length = 256\n",
        "    batch_size = 1\n",
        "    learning_rate = 2e-5\n",
        "    num_epochs = 1\n",
        "    json_file = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print(f\"ğŸ”§ Memory optimize ayarlar:\")\n",
        "print(f\"   Max length: {config.max_length}\")\n",
        "print(f\"   Batch size: {config.batch_size}\")\n",
        "print(f\"ğŸ’¾ Mevcut GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "\n",
        "# Dataset class\n",
        "class InsuranceDataset(Dataset):\n",
        "    def __init__(self, json_file_path, tokenizer, max_length=256):\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        print(f\"ğŸ“Š Dataset: {len(self.data)} Ã¶rnek yÃ¼klendi\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        question = item['input'].strip()\n",
        "        answer = item['output'].strip()\n",
        "        full_text = f\"Soru: {question}\\nCevap: {answer}\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            full_text, truncation=True, padding='max_length',\n",
        "            max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "print(\"âœ… Config ve Dataset hazÄ±r!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnfcJwfwN-NJ",
        "outputId": "a1eab729-d909-4da2-fee5-9f39eab58746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Memory optimize ayarlar:\n",
            "   Max length: 256\n",
            "   Batch size: 1\n",
            "ğŸ’¾ Mevcut GPU Memory: 0.00 GB\n",
            "âœ… Config ve Dataset hazÄ±r!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ng0Hq7waJd"
      },
      "source": [
        "\n",
        "# BÃ–LÃœM 3: DATASET CLASS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eZCMWmqf37u",
        "outputId": "1ed24e21-09f2-4643-acbe-d3d7d6d56bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset class hazÄ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class InsuranceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Sigorta Q&A verilerini JSON'dan okuyup PyTorch Dataset'e Ã§eviren class\n",
        "    Beklenen format: {\"input\": \"soru\", \"output\": \"cevap\"}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, json_file_path, tokenizer, max_length=512):\n",
        "        print(f\"ğŸ“‚ JSON dosyasÄ± okunuyor: {json_file_path}\")\n",
        "\n",
        "        # JSON dosyasÄ±nÄ± oku\n",
        "        try:\n",
        "            with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "                self.data = json.load(f)\n",
        "            print(f\"âœ… Toplam {len(self.data)} veri yÃ¼klendi\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âŒ Dosya bulunamadÄ±: {json_file_path}\")\n",
        "            raise\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"âŒ JSON format hatasÄ±: {e}\")\n",
        "            raise\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Veri formatÄ±nÄ± kontrol et\n",
        "        if len(self.data) > 0:\n",
        "            sample = self.data[0]\n",
        "            print(f\"ğŸ“‹ Veri anahtarlarÄ±: {list(sample.keys())}\")\n",
        "\n",
        "            # Format kontrolÃ¼\n",
        "            if 'input' in sample and 'output' in sample:\n",
        "                print(f\"âœ… DoÄŸru format - Ã–rnek:\")\n",
        "                print(f\"   Soru: {sample['input'][:50]}...\")\n",
        "                print(f\"   Cevap: {sample['output'][:50]}...\")\n",
        "            else:\n",
        "                print(\"âš ï¸  'input' ve 'output' anahtarlarÄ± bulunamadÄ±!\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Veriyi tokenize ederek dÃ¶ndÃ¼r\"\"\"\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Soru-cevap formatÄ±nÄ± oluÅŸtur\n",
        "        question = item['input'].strip()\n",
        "        answer = item['output'].strip()\n",
        "\n",
        "        # GPT iÃ§in training formatÄ±\n",
        "        full_text = f\"Soru: {question}\\nCevap: {answer}\"\n",
        "\n",
        "        # Tokenize et\n",
        "        encoding = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'question': question,  # Debug iÃ§in\n",
        "            'answer': answer       # Debug iÃ§in\n",
        "        }\n",
        "\n",
        "print(\"âœ… Dataset class hazÄ±r!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6cUjslPf6aP"
      },
      "source": [
        "# BÃ–LÃœM 4: TOKENIZER SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzrSRXSuf6HD",
        "outputId": "56d9e717-c9e5-45c3-b6fe-4d97c396fdfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Tokenizer setup hazÄ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def setup_tokenizer(model_name):\n",
        "    \"\"\"\n",
        "    HuggingFace tokenizer'Ä± yÃ¼kle ve ayarla\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ”¤ Tokenizer yÃ¼kleniyor: {model_name}\")\n",
        "\n",
        "    # GPT2 tokenizer yÃ¼kle\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Padding token ayarla (GPT2'de yoktur)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(\"âœ… Padding token ayarlandÄ±\")\n",
        "\n",
        "    # Tokenizer bilgilerini gÃ¶ster\n",
        "    print(f\"ğŸ“Š Vocabulary boyutu: {len(tokenizer):,}\")\n",
        "    print(f\"ğŸ”š EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n",
        "    print(f\"ğŸ“ PAD token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def test_tokenizer(tokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizer'Ä± test et\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ§ª TOKENIZER TEST EDÄ°LÄ°YOR...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Test metinleri\n",
        "    test_texts = [\n",
        "        \"Sigorta poliÃ§esi nedir?\",\n",
        "        \"Soru: AktÃ¼er kimdir?\\nCevap: SigortacÄ±lÄ±k tekniÄŸi uzmanÄ±dÄ±r.\"\n",
        "    ]\n",
        "\n",
        "    for i, text in enumerate(test_texts, 1):\n",
        "        print(f\"\\nğŸ“ Test {i}: {text}\")\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = tokenizer.encode(text)\n",
        "        print(f\"ğŸ”¢ Token sayÄ±sÄ±: {len(tokens)}\")\n",
        "        print(f\"ğŸ”¢ Token IDs: {tokens}\")\n",
        "\n",
        "        # Decode\n",
        "        decoded = tokenizer.decode(tokens)\n",
        "        print(f\"ğŸ“– Decode: {decoded}\")\n",
        "\n",
        "        # EÅŸitlik kontrolÃ¼\n",
        "        if text == decoded:\n",
        "            print(\"âœ… Encode/Decode baÅŸarÄ±lÄ±\")\n",
        "        else:\n",
        "            print(\"âš ï¸  Decode farklÄ±\")\n",
        "\n",
        "    print(\"âœ… Tokenizer test tamamlandÄ±!\")\n",
        "    return True\n",
        "\n",
        "# Tokenizer'Ä± yÃ¼kle ve test et\n",
        "def load_and_test_tokenizer(config):\n",
        "    \"\"\"\n",
        "    Tokenizer yÃ¼kle ve test et\n",
        "    \"\"\"\n",
        "    tokenizer = setup_tokenizer(config.model_name)\n",
        "    test_tokenizer(tokenizer)\n",
        "    return tokenizer\n",
        "\n",
        "print(\"âœ… Tokenizer setup hazÄ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGDSfmx3Br6i",
        "outputId": "93e91b6d-f515-4f3b-c870-b555bc0336ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¤ Tokenizer yÃ¼kleniyor: sberbank-ai/mGPT\n",
            "ğŸ“Š Vocabulary boyutu: 100,000\n",
            "ğŸ”š EOS token: '<|endoftext|>' (ID: 5)\n",
            "ğŸ“ PAD token: '<pad>' (ID: 1)\n",
            "\n",
            "ğŸ§ª TOKENIZER TEST EDÄ°LÄ°YOR...\n",
            "----------------------------------------\n",
            "\n",
            "ğŸ“ Test 1: Sigorta poliÃ§esi nedir?\n",
            "ğŸ”¢ Token sayÄ±sÄ±: 8\n",
            "ğŸ”¢ Token IDs: [52195, 17892, 38377, 739, 3788, 320, 13522, 37]\n",
            "ğŸ“– Decode: Sigorta poliÃ§esi nedir?\n",
            "âœ… Encode/Decode baÅŸarÄ±lÄ±\n",
            "\n",
            "ğŸ“ Test 2: Soru: AktÃ¼er kimdir?\n",
            "Cevap: SigortacÄ±lÄ±k tekniÄŸi uzmanÄ±dÄ±r.\n",
            "ğŸ”¢ Token sayÄ±sÄ±: 25\n",
            "ğŸ”¢ Token IDs: [57, 28073, 32, 15814, 455, 263, 24030, 46055, 37, 205, 41, 836, 511, 32, 19014, 17892, 73, 50345, 6020, 1266, 8704, 11412, 1067, 58753, 20]\n",
            "ğŸ“– Decode: Soru: AktÃ¼er kimdir?\n",
            "Cevap: SigortacÄ±lÄ±k tekniÄŸi uzmanÄ±dÄ±r.\n",
            "âœ… Encode/Decode baÅŸarÄ±lÄ±\n",
            "âœ… Tokenizer test tamamlandÄ±!\n"
          ]
        }
      ],
      "source": [
        "# Tokenizer'Ä± gerÃ§ekten yÃ¼kle ve test et\n",
        "tokenizer = load_and_test_tokenizer(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roEFNdUXf7Q6"
      },
      "source": [
        "# BÃ–LÃœM 5: MODEL CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH3pLFrWf61H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e06f91ee-d77c-46f2-fe36-71a87ac13e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model class hazÄ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class InsuranceGPTModel:\n",
        "    \"\"\"\n",
        "    HuggingFace GPT2 tabanlÄ± sigorta chatbot modeli\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, tokenizer):\n",
        "        \"\"\"\n",
        "        Model ve optimizer'Ä± initialize et\n",
        "        \"\"\"\n",
        "        print(f\"ğŸ¤– Model yÃ¼kleniyor: {config.model_name}\")\n",
        "\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # HuggingFace GPT2 model yÃ¼kle\n",
        "        try:\n",
        "            self.model = GPT2LMHeadModel.from_pretrained(\n",
        "                config.model_name,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Model yÃ¼kleme hatasÄ±: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Token embedding'i resize et\n",
        "        self.model.resize_token_embeddings(len(tokenizer))\n",
        "        print(f\"ğŸ“ Token embeddings resize edildi: {len(tokenizer)}\")\n",
        "\n",
        "        # Modeli GPU/CPU'ya taÅŸÄ±\n",
        "        self.model.to(device)\n",
        "        print(f\"ğŸ“± Model {device} cihazÄ±na taÅŸÄ±ndÄ±\")\n",
        "\n",
        "        # Model istatistikleri\n",
        "        self._print_model_stats()\n",
        "\n",
        "        # Optimizer hazÄ±rla\n",
        "        self._setup_optimizer()\n",
        "\n",
        "    def _print_model_stats(self):\n",
        "        \"\"\"Model istatistiklerini yazdÄ±r\"\"\"\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "        print(f\"ğŸ“Š Toplam parametre: {total_params:,}\")\n",
        "        print(f\"ğŸ“Š EÄŸitilebilir parametre: {trainable_params:,}\")\n",
        "        print(f\"ğŸ“Š Model boyutu: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
        "\n",
        "    def _setup_optimizer(self):\n",
        "        \"\"\"Optimizer'Ä± ayarla\"\"\"\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=self.config.learning_rate,\n",
        "            weight_decay=0.01,  # Regularization\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8\n",
        "        )\n",
        "        print(\"âœ… Optimizer hazÄ±rlandÄ±\")\n",
        "\n",
        "    def generate_answer(self, question, max_length=150):\n",
        "        \"\"\"\n",
        "        Soruya cevap Ã¼ret\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Prompt oluÅŸtur\n",
        "        prompt = f\"Soru: {question}\\nCevap:\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer.encode(\n",
        "            prompt,\n",
        "            return_tensors='pt',\n",
        "            add_special_tokens=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Cevap Ã¼ret\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=len(inputs[0]) + max_length,\n",
        "                temperature=self.config.temperature,\n",
        "                do_sample=True,\n",
        "                top_p=self.config.top_p,\n",
        "                top_k=self.config.top_k,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.2,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "\n",
        "        # Decode ve temizle\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer = full_response.replace(prompt, \"\").strip()\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def save_model(self, save_path=None):\n",
        "        \"\"\"Model ve tokenizer'Ä± kaydet\"\"\"\n",
        "        if save_path is None:\n",
        "            save_path = self.config.save_path\n",
        "\n",
        "        print(f\"ğŸ’¾ Model kaydediliyor: {save_path}\")\n",
        "\n",
        "        # KlasÃ¶r oluÅŸtur\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        # Model ve tokenizer kaydet\n",
        "        self.model.save_pretrained(save_path)\n",
        "        self.tokenizer.save_pretrained(save_path)\n",
        "\n",
        "        print(\"âœ… Model baÅŸarÄ±yla kaydedildi!\")\n",
        "\n",
        "    def load_model(self, model_path=None):\n",
        "        \"\"\"KaydedilmiÅŸ modeli yÃ¼kle\"\"\"\n",
        "        if model_path is None:\n",
        "            model_path = self.config.save_path\n",
        "\n",
        "        print(f\"ğŸ“‚ Model yÃ¼kleniyor: {model_path}\")\n",
        "\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.model.to(device)\n",
        "\n",
        "        print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "\n",
        "def test_model_basic(model):\n",
        "    \"\"\"\n",
        "    Model'in temel Ã§alÄ±ÅŸmasÄ±nÄ± test et\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ§ª MODEL TEMEL TEST...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    test_questions = [\n",
        "        \"AktÃ¼erler Sicili kimde tutulur?\",\n",
        "        \"Sigorta ÅŸirketi nedir?\"\n",
        "    ]\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\nâ“ Soru: {question}\")\n",
        "        try:\n",
        "            answer = model.generate_answer(question, max_length=50)\n",
        "            print(f\"ğŸ¤– Cevap: {answer}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Hata: {e}\")\n",
        "\n",
        "    print(\"âœ… Model temel test tamamlandÄ±!\")\n",
        "\n",
        "print(\"âœ… Model class hazÄ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGyRbM4FB-pL"
      },
      "outputs": [],
      "source": [
        "# Google Drive'Ä± mount et (eÄŸer etmediysen)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Model class'Ä±nÄ± test et\n",
        "model = InsuranceGPTModel(config, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6UUotcEf8jj"
      },
      "source": [
        "# BÃ–LÃœM 6: TRAINING FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pThSTEgef8Ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1c0c00-8413-42d8-db76-0149e7b9f040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training functions hazÄ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, epoch_num, config):\n",
        "    \"\"\"\n",
        "    Tek epoch eÄŸitimi\n",
        "    \"\"\"\n",
        "    model.model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    print(f\"ğŸ¯ Epoch {epoch_num+1}/{config.num_epochs} baÅŸlÄ±yor...\")\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        dataloader,\n",
        "        desc=f'Epoch {epoch_num+1}',\n",
        "        leave=True\n",
        "    )\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        # Veriyi GPU'ya taÅŸÄ±\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=input_ids  # Language modeling iÃ§in\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        model.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (exploding gradient'larÄ± Ã¶nle)\n",
        "        torch.nn.utils.clip_grad_norm_(model.model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Optimizer step\n",
        "        model.optimizer.step()\n",
        "\n",
        "        # Loss tracking\n",
        "        total_loss += loss.item()\n",
        "        current_avg_loss = total_loss / (batch_idx + 1)\n",
        "\n",
        "        # Progress bar gÃ¼ncelle\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Avg': f'{current_avg_loss:.4f}',\n",
        "            'GPU': f'{torch.cuda.memory_allocated()/1024**3:.1f}GB' if torch.cuda.is_available() else 'CPU'\n",
        "        })\n",
        "\n",
        "        # Memory temizliÄŸi (her 10 batch'te bir)\n",
        "        if batch_idx % 10 == 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    epoch_avg_loss = total_loss / num_batches\n",
        "    print(f\"âœ… Epoch {epoch_num+1} tamamlandÄ± - Avg Loss: {epoch_avg_loss:.4f}\")\n",
        "\n",
        "    return epoch_avg_loss\n",
        "\n",
        "def full_training(model, dataset, config):\n",
        "    \"\"\"\n",
        "    Tam eÄŸitim dÃ¶ngÃ¼sÃ¼\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸš€ EÄÄ°TÄ°M BAÅLIYOR...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # DataLoader oluÅŸtur\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ“Š Toplam batch sayÄ±sÄ±: {len(dataloader)}\")\n",
        "    print(f\"ğŸ“Š Her epoch'ta ~{len(dataset)} Ã¶rnek\")\n",
        "\n",
        "    # EÄŸitim dÃ¶ngÃ¼sÃ¼\n",
        "    losses = []\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f\"\\n{'='*20} EPOCH {epoch+1}/{config.num_epochs} {'='*20}\")\n",
        "\n",
        "        # Epoch eÄŸitimi\n",
        "        epoch_loss = train_one_epoch(model, dataloader, epoch, config)\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "        # En iyi model tracking\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            print(f\"ğŸ† Yeni en iyi loss: {best_loss:.4f}\")\n",
        "\n",
        "        # Memory temizliÄŸi\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\nğŸ EÄÄ°TÄ°M TAMAMLANDI!\")\n",
        "    print(f\"ğŸ“ˆ Son loss: {losses[-1]:.4f}\")\n",
        "    print(f\"ğŸ† En iyi loss: {best_loss:.4f}\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "def plot_training_loss(losses):\n",
        "    \"\"\"\n",
        "    Training loss grafiÄŸini Ã§iz\n",
        "    \"\"\"\n",
        "    if len(losses) == 0:\n",
        "        print(\"âš ï¸  Ã‡izilebilecek loss verisi yok\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(losses) + 1), losses, 'b-', marker='o', linewidth=2, markersize=8)\n",
        "    plt.title('Training Loss', fontsize=16, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(range(1, len(losses) + 1))\n",
        "\n",
        "    # Loss deÄŸerlerini grafikte gÃ¶ster\n",
        "    for i, loss in enumerate(losses):\n",
        "        plt.annotate(f'{loss:.4f}', (i+1, loss), textcoords=\"offset points\",\n",
        "                    xytext=(0,10), ha='center', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"ğŸ“Š Loss grafiÄŸi Ã§izildi!\")\n",
        "\n",
        "print(\"âœ… Training functions hazÄ±r!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNFWb6kgRoi"
      },
      "source": [
        "# BÃ–LÃœM 7: TEST FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOjhTtw-gRXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f429023-0467-4fd9-934b-61ab2c3c5357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Test functions hazÄ±r!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def test_dataset_loading(json_file_path, tokenizer, config):\n",
        "    \"\"\"\n",
        "    Dataset yÃ¼kleme ve tokenizasyon testleri\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ§ª DATASET YÃœKLEMESÄ° TEST EDÄ°LÄ°YOR...\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    try:\n",
        "        # Dataset oluÅŸtur\n",
        "        dataset = InsuranceDataset(json_file_path, tokenizer, config.max_length)\n",
        "\n",
        "        if len(dataset) == 0:\n",
        "            print(\"âŒ Dataset boÅŸ!\")\n",
        "            return False\n",
        "\n",
        "        # Ä°lk Ã¶rneÄŸi kontrol et\n",
        "        print(f\"âœ… Dataset boyutu: {len(dataset)}\")\n",
        "\n",
        "        # Tokenizasyon testi\n",
        "        sample = dataset[0]\n",
        "        print(f\"âœ… Input IDs shape: {sample['input_ids'].shape}\")\n",
        "        print(f\"âœ… Attention mask shape: {sample['attention_mask'].shape}\")\n",
        "\n",
        "        # Ã–rnek decode\n",
        "        decoded = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
        "        print(f\"ğŸ“ Ä°lk Ã¶rnek: {decoded[:100]}...\")\n",
        "\n",
        "        # DataLoader testi\n",
        "        dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
        "        batch = next(iter(dataloader))\n",
        "        print(f\"âœ… Batch test - Input shape: {batch['input_ids'].shape}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Dataset test hatasÄ±: {e}\")\n",
        "        return False\n",
        "\n",
        "def test_model_generation(model, test_questions):\n",
        "    \"\"\"\n",
        "    Model text generation testleri\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ§ª MODEL GENERATÄ°ON TEST EDÄ°LÄ°YOR...\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"\\nğŸ”¸ Test {i}/{len(test_questions)}\")\n",
        "        print(f\"â“ Soru: {question}\")\n",
        "\n",
        "        try:\n",
        "            # Cevap Ã¼ret\n",
        "            answer = model.generate_answer(question, max_length=80)\n",
        "            print(f\"ğŸ¤– Cevap: {answer}\")\n",
        "\n",
        "            # Token analizi\n",
        "            prompt = f\"Soru: {question}\\nCevap:\"\n",
        "            tokens = model.tokenizer.encode(prompt)\n",
        "            print(f\"ğŸ”¢ Prompt token sayÄ±sÄ±: {len(tokens)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Generation hatasÄ±: {e}\")\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "def compare_before_after_training(model, test_questions):\n",
        "    \"\"\"\n",
        "    EÄŸitim Ã¶ncesi ve sonrasÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ“Š EÄÄ°TÄ°M Ã–NCESÄ°/SONRASI KARÅILAÅTIRMA\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for question in test_questions:\n",
        "        print(f\"\\nâ“ Soru: {question}\")\n",
        "\n",
        "        # EÄŸitim Ã¶ncesi cevap (bu fonksiyon eÄŸitim sonrasÄ± Ã§aÄŸrÄ±lÄ±r)\n",
        "        answer = model.generate_answer(question, max_length=60)\n",
        "\n",
        "        results[question] = {\n",
        "            'after_training': answer\n",
        "        }\n",
        "\n",
        "        print(f\"ğŸ¤– EÄŸitim sonrasÄ±: {answer}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    return results\n",
        "\n",
        "def evaluate_model_performance(model, dataset, sample_size=10):\n",
        "    \"\"\"\n",
        "    Model performansÄ±nÄ± deÄŸerlendir\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ“ˆ MODEL PERFORMANS DEÄERLENDÄ°RME (Sample: {sample_size})\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Random sample al\n",
        "    import random\n",
        "    indices = random.sample(range(len(dataset)), min(sample_size, len(dataset)))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        sample = dataset[idx]\n",
        "        question = sample['question']\n",
        "        expected_answer = sample['answer']\n",
        "\n",
        "        print(f\"\\nğŸ“ Test {i+1}/{sample_size}\")\n",
        "        print(f\"â“ Soru: {question}\")\n",
        "        print(f\"âœ… Beklenen: {expected_answer[:50]}...\")\n",
        "\n",
        "        # Model cevabÄ±\n",
        "        generated_answer = model.generate_answer(question, max_length=60)\n",
        "        print(f\"ğŸ¤– Ãœretilen: {generated_answer[:50]}...\")\n",
        "\n",
        "        # Basit benzerlik skoru (kelime overlap)\n",
        "        expected_words = set(expected_answer.lower().split())\n",
        "        generated_words = set(generated_answer.lower().split())\n",
        "\n",
        "        if len(expected_words) > 0:\n",
        "            overlap = len(expected_words.intersection(generated_words))\n",
        "            similarity = overlap / len(expected_words)\n",
        "        else:\n",
        "            similarity = 0\n",
        "\n",
        "        print(f\"ğŸ“Š Benzerlik: {similarity:.2f}\")\n",
        "\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'expected': expected_answer,\n",
        "            'generated': generated_answer,\n",
        "            'similarity': similarity\n",
        "        })\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # Ortalama performans\n",
        "    avg_similarity = sum(r['similarity'] for r in results) / len(results)\n",
        "    print(f\"\\nğŸ† ORTALAMA BENZERLÄ°K: {avg_similarity:.3f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def interactive_test_mode(model):\n",
        "    \"\"\"\n",
        "    Ä°nteraktif test modu\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ® Ä°NTERAKTÄ°F TEST MODU\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Sigorta ile ilgili sorularÄ±nÄ±zÄ± sorun!\")\n",
        "    print(\"Ã‡Ä±kmak iÃ§in 'quit', 'exit' veya 'Ã§Ä±k' yazÄ±n.\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            question = input(\"\\nâ“ Sigorta sorunuz: \").strip()\n",
        "\n",
        "            # Ã‡Ä±kÄ±ÅŸ kontrolleri\n",
        "            if question.lower() in ['quit', 'exit', 'Ã§Ä±k', 'q']:\n",
        "                print(\"ğŸ‘‹ Ä°nteraktif test sonlandÄ±rÄ±lÄ±yor...\")\n",
        "                break\n",
        "\n",
        "            if not question:\n",
        "                print(\"âš ï¸  BoÅŸ soru girdiniz!\")\n",
        "                continue\n",
        "\n",
        "            # Tokenizasyon bilgisi\n",
        "            tokens = model.tokenizer.encode(f\"Soru: {question}\\nCevap:\")\n",
        "            print(f\"ğŸ”¢ Token sayÄ±sÄ±: {len(tokens)}\")\n",
        "\n",
        "            # Cevap Ã¼ret\n",
        "            print(\"ğŸ¤– DÃ¼ÅŸÃ¼nÃ¼yorum...\")\n",
        "            answer = model.generate_answer(question, max_length=100)\n",
        "\n",
        "            print(f\"ğŸ¤– Cevap: {answer}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nğŸ‘‹ KullanÄ±cÄ± Ã§Ä±kÄ±ÅŸÄ± yapÄ±ldÄ±.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Hata: {e}\")\n",
        "\n",
        "# Test soru setleri\n",
        "BASIC_TEST_QUESTIONS = [\n",
        "    \"AktÃ¼erler Sicili kimde tutulur?\",\n",
        "    \"Brokerlik ruhsatÄ± kimden alÄ±nÄ±r?\",\n",
        "    \"Minimum garanti fonu nedir?\"\n",
        "]\n",
        "\n",
        "ADVANCED_TEST_QUESTIONS = [\n",
        "    \"Sigorta ÅŸirketleri hangi branÅŸlarda faaliyet gÃ¶sterebilir?\",\n",
        "    \"Teknik karÅŸÄ±lÄ±klar nelerdir?\",\n",
        "    \"Zorunlu sigortalar kimler tarafÄ±ndan ihdas edilir?\",\n",
        "    \"GÃ¼vence HesabÄ± ne iÅŸe yarar?\",\n",
        "    \"Sigorta eksperi kimdir?\"\n",
        "]\n",
        "\n",
        "def run_all_tests(model, dataset):\n",
        "    \"\"\"\n",
        "    TÃ¼m testleri Ã§alÄ±ÅŸtÄ±r\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ§ª TÃœM TESTLER Ã‡ALIÅTIRILIYOR...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Temel generation testleri\n",
        "    test_model_generation(model, BASIC_TEST_QUESTIONS)\n",
        "\n",
        "    # Performans deÄŸerlendirmesi\n",
        "    evaluate_model_performance(model, dataset, sample_size=5)\n",
        "\n",
        "    # Ä°nteraktif mod\n",
        "    interactive_test_mode(model)\n",
        "\n",
        "print(\"âœ… Test functions hazÄ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ofqTrZBECgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3c1f00-5d38-41fd-dcef-80cae8a247a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Yeni dosya yolu: /content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\n",
            "\n",
            "ğŸ§ª DATASET YÃœKLEMESÄ° TEST EDÄ°LÄ°YOR...\n",
            "---------------------------------------------\n",
            "ğŸ“‚ JSON dosyasÄ± okunuyor: /content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\n",
            "âœ… Toplam 89 veri yÃ¼klendi\n",
            "ğŸ“‹ Veri anahtarlarÄ±: ['input', 'output']\n",
            "âœ… DoÄŸru format - Ã–rnek:\n",
            "   Soru: Bu kanunun amacÄ± nedir?...\n",
            "   Cevap: Bu Kanunun amacÄ±, Ã¼lkemiz sigortacÄ±lÄ±ÄŸÄ±nÄ±n geliÅŸti...\n",
            "âœ… Dataset boyutu: 89\n",
            "âœ… Input IDs shape: torch.Size([256])\n",
            "âœ… Attention mask shape: torch.Size([256])\n",
            "ğŸ“ Ä°lk Ã¶rnek: Soru: Bu kanunun amacÄ± nedir?\n",
            "Cevap: Bu Kanunun amacÄ±, Ã¼lkemiz sigortacÄ±lÄ±ÄŸÄ±nÄ±n geliÅŸtirilmesini saÄŸ...\n",
            "âœ… Batch test - Input shape: torch.Size([2, 256])\n",
            "Dataset test sonucu: True\n"
          ]
        }
      ],
      "source": [
        "# DoÄŸru dosya yolunu config'e kaydet\n",
        "config.json_file = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\"\n",
        "\n",
        "print(f\"âœ… Yeni dosya yolu: {config.json_file}\")\n",
        "\n",
        "# Åimdi dataset testini tekrar yapalÄ±m\n",
        "dataset_test_result = test_dataset_loading(config.json_file, tokenizer, config)\n",
        "print(f\"Dataset test sonucu: {dataset_test_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPYI0v4uzJbe"
      },
      "source": [
        "\n",
        "# BÃ–LÃœM 8: MAIN EXECUTION PIPELINE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2-SkoSeBhb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1c5470-44fa-48ec-f498-64e67df79d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Main execution pipeline hazÄ±r!\n",
            "\n",
            "ğŸ“‹ KULLANIM:\n",
            "â€¢ quick_test() - HÄ±zlÄ± test (1 epoch)\n",
            "â€¢ full_training_run() - Tam eÄŸitim\n",
            "â€¢ only_test_model() - Sadece test\n",
            "â€¢ main_pipeline(quick_mode=True/False) - Manual kontrol\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def setup_google_drive():\n",
        "    \"\"\"\n",
        "    Google Drive'Ä± mount et\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“ Google Drive kontrol ediliyor...\")\n",
        "\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"ğŸ”— Google Drive mount ediliyor...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"âœ… Google Drive mount edildi!\")\n",
        "    else:\n",
        "        print(\"âœ… Google Drive zaten mount edilmiÅŸ!\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def check_file_exists(file_path):\n",
        "    \"\"\"\n",
        "    Dosya varlÄ±ÄŸÄ±nÄ± kontrol et\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ“‚ Dosya kontrol ediliyor: {file_path}\")\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
        "        print(f\"âœ… Dosya bulundu! Boyut: {file_size:.1f} KB\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"âŒ Dosya bulunamadÄ±: {file_path}\")\n",
        "        return False\n",
        "\n",
        "def main_pipeline(quick_mode=False):\n",
        "    \"\"\"\n",
        "    Ana eÄŸitim pipeline'Ä±\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ SÄ°GORTA GPT EÄÄ°TÄ°M PÄ°PELÄ°NE BAÅLIYOR!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # HÄ±zlÄ± mod ayarlarÄ±\n",
        "        if quick_mode:\n",
        "            print(\"âš¡ HIZLI MOD AKTÄ°F - Test iÃ§in optimize edildi\")\n",
        "            config.num_epochs = 1\n",
        "            config.batch_size = 2\n",
        "\n",
        "        # ADIM 1: Google Drive Setup\n",
        "        print(f\"\\n{'='*15} ADIM 1: DRIVE SETUP {'='*15}\")\n",
        "        setup_google_drive()\n",
        "\n",
        "        # ADIM 2: Dosya Kontrolleri\n",
        "        print(f\"\\n{'='*15} ADIM 2: DOSYA KONTROL {'='*15}\")\n",
        "        if not check_file_exists(config.json_file):\n",
        "            print(\"âŒ Pipeline durduruluyor - JSON dosyasÄ± bulunamadÄ±!\")\n",
        "            return False\n",
        "\n",
        "        # ADIM 3: Tokenizer YÃ¼kleme\n",
        "        print(f\"\\n{'='*15} ADIM 3: TOKENIZER {'='*15}\")\n",
        "        tokenizer = load_and_test_tokenizer(config)\n",
        "\n",
        "        # ADIM 4: Dataset Test\n",
        "        print(f\"\\n{'='*15} ADIM 4: DATASET TEST {'='*15}\")\n",
        "        if not test_dataset_loading(config.json_file, tokenizer, config):\n",
        "            print(\"âŒ Pipeline durduruluyor - Dataset hatasÄ±!\")\n",
        "            return False\n",
        "\n",
        "        # ADIM 5: Model YÃ¼kleme\n",
        "        print(f\"\\n{'='*15} ADIM 5: MODEL YÃœKLEME {'='*15}\")\n",
        "        model = InsuranceGPTModel(config, tokenizer)\n",
        "\n",
        "        # ADIM 6: EÄŸitim Ã–ncesi Test\n",
        "        print(f\"\\n{'='*15} ADIM 6: Ã–NCESÄ° TEST {'='*15}\")\n",
        "        print(\"ğŸ“ EÄŸitim Ã¶ncesi model cevaplarÄ±:\")\n",
        "        test_model_generation(model, BASIC_TEST_QUESTIONS[:2])\n",
        "\n",
        "        # ADIM 7: Dataset YÃ¼kleme\n",
        "        print(f\"\\n{'='*15} ADIM 7: DATASET YÃœKLEME {'='*15}\")\n",
        "        dataset = InsuranceDataset(config.json_file, tokenizer, config.max_length)\n",
        "        print(f\"âœ… {len(dataset)} eÄŸitim Ã¶rneÄŸi hazÄ±r!\")\n",
        "\n",
        "        # ADIM 8: Model EÄŸitimi\n",
        "        print(f\"\\n{'='*15} ADIM 8: MODEL EÄÄ°TÄ°MÄ° {'='*15}\")\n",
        "        losses = full_training(model, dataset, config)\n",
        "\n",
        "        # ADIM 9: Loss GrafiÄŸi\n",
        "        print(f\"\\n{'='*15} ADIM 9: LOSS GRAFÄ°ÄÄ° {'='*15}\")\n",
        "        plot_training_loss(losses)\n",
        "\n",
        "        # ADIM 10: EÄŸitim SonrasÄ± Test\n",
        "        print(f\"\\n{'='*15} ADIM 10: SONRASI TEST {'='*15}\")\n",
        "        print(\"ğŸ“ EÄŸitim sonrasÄ± model cevaplarÄ±:\")\n",
        "        test_model_generation(model, BASIC_TEST_QUESTIONS)\n",
        "\n",
        "        # ADIM 11: Model Kaydetme\n",
        "        print(f\"\\n{'='*15} ADIM 11: MODEL KAYDET {'='*15}\")\n",
        "        model.save_model()\n",
        "\n",
        "        # ADIM 12: Performans DeÄŸerlendirme\n",
        "        print(f\"\\n{'='*15} ADIM 12: PERFORMANS {'='*15}\")\n",
        "        evaluate_model_performance(model, dataset, sample_size=3)\n",
        "\n",
        "        print(\"\\nğŸ‰ TÃœM ADIMLAR BAÅARIYLA TAMAMLANDI!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Ä°nteraktif mod teklifi\n",
        "        user_input = input(\"\\nğŸ® Ä°nteraktif test moduna geÃ§mek istiyor musunuz? (e/h): \").lower()\n",
        "        if user_input in ['e', 'evet', 'y', 'yes']:\n",
        "            interactive_test_mode(model)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ PÄ°PELÄ°NE HATASI: {e}\")\n",
        "        import traceback\n",
        "        print(\"\\nDetaylÄ± hata:\")\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"\n",
        "    HÄ±zlÄ± test modu - 1 epoch, kÃ¼Ã§Ã¼k batch\n",
        "    \"\"\"\n",
        "    print(\"âš¡ HIZLI TEST MODU BAÅLIYOR...\")\n",
        "    return main_pipeline(quick_mode=True)\n",
        "\n",
        "def full_training_run():\n",
        "    \"\"\"\n",
        "    Tam eÄŸitim modu - tÃ¼m epoch'lar\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ TAM EÄÄ°TÄ°M MODU BAÅLIYOR...\")\n",
        "    return main_pipeline(quick_mode=False)\n",
        "\n",
        "def only_test_model(model_path=None):\n",
        "    \"\"\"\n",
        "    Sadece kaydedilmiÅŸ modeli test et\n",
        "    \"\"\"\n",
        "    print(\"ğŸ§ª SADECE MODEL TEST MODU\")\n",
        "\n",
        "    if model_path is None:\n",
        "        model_path = config.save_path\n",
        "\n",
        "    # Tokenizer ve model yÃ¼kle\n",
        "    tokenizer = load_and_test_tokenizer(config)\n",
        "    model = InsuranceGPTModel(config, tokenizer)\n",
        "    model.load_model(model_path)\n",
        "\n",
        "    # Testleri Ã§alÄ±ÅŸtÄ±r\n",
        "    test_model_generation(model, ADVANCED_TEST_QUESTIONS)\n",
        "    interactive_test_mode(model)\n",
        "\n",
        "# KullanÄ±m kÄ±lavuzu\n",
        "print(\"âœ… Main execution pipeline hazÄ±r!\")\n",
        "print(\"\\nğŸ“‹ KULLANIM:\")\n",
        "print(\"â€¢ quick_test() - HÄ±zlÄ± test (1 epoch)\")\n",
        "print(\"â€¢ full_training_run() - Tam eÄŸitim\")\n",
        "print(\"â€¢ only_test_model() - Sadece test\")\n",
        "print(\"â€¢ main_pipeline(quick_mode=True/False) - Manual kontrol\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIwPb1fxEqGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc94959-ecde-484a-a68a-1c7198f24612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¤ Tokenizer yÃ¼kleniyor: sberbank-ai/mGPT\n",
            "ğŸ¤– Model yÃ¼kleniyor: sberbank-ai/mGPT\n",
            "ğŸ“Š Dataset oluÅŸturuluyor...\n",
            "âœ… 89 Ã¶rnek hazÄ±r!\n",
            "\n",
            "ğŸ§ª EÄÄ°TÄ°M Ã–NCESÄ° TEST:\n",
            "â“ AktÃ¼erler Sicili kimde tutulur?\n",
            "ğŸ¤– AktÃ¼erler Sicili, AktÃ¼erlik mesleÄŸinin gerektirdiÄŸi bilgi ve beceriler kazanmak amacÄ±yla, meslek mensuplarÄ± arasÄ±nda yapÄ±lan araÅŸtÄ±rmalar sonucunda, gerekli ÅŸartlara sahip\n",
            "------------------------------\n",
            "â“ Brokerlik ruhsatÄ± kimden alÄ±nÄ±r?\n",
            "ğŸ¤– Brokerlik ruhsatÄ±, brokerlere verilir. Bu ruhsat, brokerlerce verilir. Bu ruhsatlar, brokerlerin genel gÃ¶revleri arasÄ±nda yer alÄ±r.\n",
            "Soru: KÄ±saca ne iÅŸ yapar\n",
            "------------------------------\n",
            "âœ… TÃ¼m setup tamamlandÄ±! Åimdi eÄŸitime geÃ§ebiliriz.\n"
          ]
        }
      ],
      "source": [
        "# TÃœM KODLARI TEK HÃœCREDE TOPARLAYALIM\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Config\n",
        "class Config:\n",
        "    model_name = \"sberbank-ai/mGPT\"\n",
        "    max_length = 512\n",
        "    batch_size = 2\n",
        "    learning_rate = 2e-5\n",
        "    num_epochs = 1\n",
        "    json_file = \"/content/drive/MyDrive/Colab Notebooks/fine-tuning/sigorta_qa_ft.json\"\n",
        "\n",
        "config = Config()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Dataset Class\n",
        "class InsuranceDataset(Dataset):\n",
        "    def __init__(self, json_file_path, tokenizer, max_length=512):\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        question = item['input'].strip()\n",
        "        answer = item['output'].strip()\n",
        "        full_text = f\"Soru: {question}\\nCevap: {answer}\"\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            full_text, truncation=True, padding='max_length',\n",
        "            max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'question': question, 'answer': answer\n",
        "        }\n",
        "\n",
        "# Model ve tokenizer yÃ¼kle\n",
        "print(f\"ğŸ”¤ Tokenizer yÃ¼kleniyor: {config.model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"ğŸ¤– Model yÃ¼kleniyor: {config.model_name}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(config.model_name).to(device)\n",
        "\n",
        "# Text generation fonksiyonu\n",
        "def generate_text(model, tokenizer, prompt, max_length=50):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs, max_length=len(inputs[0]) + max_length,\n",
        "                                temperature=0.7, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
        "\n",
        "# Dataset oluÅŸtur\n",
        "print(\"ğŸ“Š Dataset oluÅŸturuluyor...\")\n",
        "dataset = InsuranceDataset(config.json_file, tokenizer, config.max_length)\n",
        "print(f\"âœ… {len(dataset)} Ã¶rnek hazÄ±r!\")\n",
        "\n",
        "# EÄŸitim Ã¶ncesi test\n",
        "print(\"\\nğŸ§ª EÄÄ°TÄ°M Ã–NCESÄ° TEST:\")\n",
        "test_questions = [\"AktÃ¼erler Sicili kimde tutulur?\", \"Brokerlik ruhsatÄ± kimden alÄ±nÄ±r?\"]\n",
        "for q in test_questions:\n",
        "    prompt = f\"Soru: {q}\\nCevap:\"\n",
        "    answer = generate_text(model, tokenizer, prompt, 50)\n",
        "    print(f\"â“ {q}\")\n",
        "    print(f\"ğŸ¤– {answer}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "print(\"âœ… TÃ¼m setup tamamlandÄ±! Åimdi eÄŸitime geÃ§ebiliriz.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL VE TOKENIZER YÃœKLEME\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(\"ğŸ”¤ Tokenizer yÃ¼kleniyor...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"âœ… Padding token ayarlandÄ±\")\n",
        "\n",
        "print(f\"ğŸ“Š Vocabulary: {len(tokenizer):,} token\")\n",
        "\n",
        "print(\"ğŸ¤– Model yÃ¼kleniyor (float16 - memory efficient)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_name,\n",
        "    torch_dtype=torch.float16  # Memory iÃ§in float16\n",
        ").to(device)\n",
        "\n",
        "print(f\"ğŸ’¾ Model yÃ¼klendi - GPU: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(model, tokenizer, prompt, max_length=40):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs,\n",
        "            max_length=len(inputs[0]) + max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
        "\n",
        "print(\"âœ… Model setup tamamlandÄ±!\")\n",
        "\n",
        "# HÄ±zlÄ± test\n",
        "print(\"\\nğŸ§ª HIZLI TEST:\")\n",
        "test_q = \"Sigorta nedir?\"\n",
        "prompt = f\"Soru: {test_q}\\nCevap:\"\n",
        "answer = generate_text(model, tokenizer, prompt, 30)\n",
        "print(f\"â“ {test_q}\")\n",
        "print(f\"ğŸ¤– {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "k2_Z64jmUef4",
        "outputId": "904ed4b9-64d5-49c4-944d-c3ed7d4485e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¤ Tokenizer yÃ¼kleniyor...\n",
            "ğŸ“Š Vocabulary: 100,000 token\n",
            "ğŸ¤– Model yÃ¼kleniyor (float16 - memory efficient)...\n",
            "ğŸ’¾ Model yÃ¼klendi - GPU: 0.00 GB\n",
            "âœ… Model setup tamamlandÄ±!\n",
            "\n",
            "ğŸ§ª HIZLI TEST:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-373971593.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mtest_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Sigorta nedir?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Soru: {test_q}\\nCevap:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"â“ {test_q}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ğŸ¤– {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-373971593.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, prompt, max_length)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2628\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2629\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2630\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2631\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3611\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3612\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3613\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1077\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             outputs = block(\n\u001b[0m\u001b[1;32m    928\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0mpast_key_values\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, past_key_value, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mfeed_forward_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;31m# residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfeed_forward_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esWCQB6GzYjL"
      },
      "source": [
        "# 12. HÄ±zlÄ± Ã‡alÄ±ÅŸtÄ±rma FonksiyonlarÄ±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlQrmYVeH15O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a338c1ac-aeee-430f-f903-60c592ce6ed9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ¯ EÄÄ°TÄ°M BAÅLIYOR!\n",
            "ğŸ“Š Epoch: 1 | Batch: 45 | LR: 2e-05\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch 1/1:   0%|          | 0/45 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# EÄÄ°TÄ°M FONKSÄ°YONU VE BAÅLATMA\n",
        "def train_model(model, dataset, config):\n",
        "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    model.train()\n",
        "    print(f\"ğŸ¯ EÄÄ°TÄ°M BAÅLIYOR!\")\n",
        "    print(f\"ğŸ“Š Epoch: {config.num_epochs} | Batch: {len(dataloader)} | LR: {config.learning_rate}\")\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{config.num_epochs}')\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'avg': f'{total_loss/(progress_bar.n+1):.4f}'\n",
        "            })\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"âœ… Epoch {epoch+1} tamamlandÄ± - Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# EÄÄ°TÄ°MÄ° BAÅLAT\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "trained_model = train_model(model, dataset, config)\n",
        "\n",
        "# EÄÄ°TÄ°M SONRASI TEST\n",
        "print(\"\\nğŸ§ª EÄÄ°TÄ°M SONRASI TEST:\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for question in test_questions:\n",
        "    prompt = f\"Soru: {question}\\nCevap:\"\n",
        "    answer = generate_text(trained_model, tokenizer, prompt, max_length=60)\n",
        "    print(f\"â“ {question}\")\n",
        "    print(f\"ğŸ¤– EÄÄ°TÄ°M SONRASI: {answer}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"ğŸ‰ EÄÄ°TÄ°M VE TEST TAMAMLANDI!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rkhqGG6xwTId",
        "6kdER9KD_U7P",
        "A3ng0Hq7waJd",
        "O6cUjslPf6aP",
        "roEFNdUXf7Q6",
        "G6UUotcEf8jj"
      ],
      "provenance": [],
      "mount_file_id": "10sA0sJ5uxFYKE5s5rPbNVCFYo9W9U2Kw",
      "authorship_tag": "ABX9TyOC3J3yrP4by75xaiSlU084",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}